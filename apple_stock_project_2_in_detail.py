# -*- coding: utf-8 -*-
"""apple_stock_project_2_in_detail.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMooh-ZgSLFS_CVFc1h0c6XFbnC4JXYu

##apple_stock_project_2_

## Cell 1: Import Libraries
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

"""## Cell 2: Upload Dataset and Load Data
# In this cell, we upload the dataset from a file stored locally or via Google Colab file upload.
# After uploading, the dataset is read into a pandas DataFrame for further processing.

"""

from google.colab import files
uploaded = files.upload()
df = pd.read_csv(next(iter(uploaded.keys())))

"""## Cell 3: EDA - Summary Statistics
# This cell displays basic information about the dataset, such as data types, non-null values, and memory usage.
# Additionally, it shows summary statistics (mean, std, min, max, etc.) for each numeric feature in the dataset.

"""

print("--- Dataset Info ---")
df.info()
print("\n--- Summary Statistics ---")
print(df.describe())

"""## Cell 4: EDA - Missing Values and Correlation
# Here, we check for any missing values in the dataset and display the total count for each column.
# We also compute the correlation matrix for the numerical features to identify relationships between them.
# The correlation matrix is visualized using a heatmap for better understanding.

"""

print("\n--- Missing Values ---")
print(df.isnull().sum())

plt.figure(figsize=(10, 6))
correlation_matrix = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""## Cell 5: Preprocess Data
# In this step, the 'Date' column is converted to a datetime format and sorted.
# We define the features ('Open', 'High', 'Low') and target variable ('Close') for the model.

"""

df['Date'] = pd.to_datetime(df['Date'], utc=True)
df.sort_values(by='Date', inplace=True)
features = ['Open', 'High', 'Low']
target = 'Close'
X = df[features]
y = df[target]

## Cell 6: Standardize Features and Apply PCA
# We standardize the feature set using StandardScaler to scale all features to a similar range.
# Then, Principal Component Analysis (PCA) is applied to reduce the dimensionality of the feature set to 2 components.

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

"""## Cell 7: Split Data into Training and Testing Sets (70:30)
# The dataset is split into training and testing sets with a 70:30 ratio.
# The training data is used to train the model, and the testing data is used to evaluate the model's performance.

"""

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)
print(f"Training Set Size: {X_train.shape[0]} samples")
print(f"Testing Set Size: {X_test.shape[0]} samples")

"""## Cell 8: Train Linear Regression Model
# We initialize a Linear Regression model and train it on the training data.
# Predictions are made on the test set after the model is trained.

"""

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_lr_pred = lr_model.predict(X_test)

"""## Cell 9: Evaluate Linear Regression Model
# In this step, the performance of the Linear Regression model is evaluated using two metrics:
# 1. Mean Squared Error (MSE), which measures the average squared difference between predicted and actual values.
# 2. R-squared (R²), which indicates how well the model explains the variance in the target variable.

"""

lr_mse = mean_squared_error(y_test, y_lr_pred)
lr_r2 = r2_score(y_test, y_lr_pred)
print("--- Linear Regression Results ---")
print(f"Linear Regression Mean Squared Error (MSE): {lr_mse}")
print(f"Linear Regression R-squared (R²): {lr_r2}")

"""## Cell 10: Visualize Linear Regression Predictions
# A scatter plot is created to visualize the relationship between the actual and predicted values for the Linear Regression model.
# A red dashed line is added to show the perfect fit (where predicted values equal actual values).

"""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_lr_pred, alpha=0.7, color='blue', label='Predicted vs Actual (Linear Regression)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red', label='Perfect Fit')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values (Linear Regression)')
plt.legend()
plt.show()

"""## Cell 11: Train Support Vector Machine Model
# Here, we train a Support Vector Machine (SVM) model using the radial basis function (RBF) kernel.
# The SVM model is trained on the same training data and used to predict the closing prices.

"""

svm_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)
svm_model.fit(X_train, y_train)
y_svm_pred = svm_model.predict(X_test)

"""## Cell 12: Evaluate Support Vector Machine Model
# The performance of the Support Vector Machine (SVM) model is evaluated using the same metrics as Linear Regression:
# Mean Squared Error (MSE) and R-squared (R²).

"""

svm_mse = mean_squared_error(y_test, y_svm_pred)
svm_r2 = r2_score(y_test, y_svm_pred)
print("--- Support Vector Machine Results ---")
print(f"SVM Mean Squared Error (MSE): {svm_mse}")
print(f"SVM R-squared (R²): {svm_r2}")

"""## Cell 13: Visualize Support Vector Machine Predictions
# A scatter plot is created to visualize the relationship between the actual and predicted values for the SVM model.
# Again, a red dashed line represents the perfect fit.

"""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_svm_pred, alpha=0.7, color='green', label='Predicted vs Actual (SVM)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red', label='Perfect Fit')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values (SVM)')
plt.legend()
plt.show()

"""## Cell 14: Moving Average Plot
# A moving average is calculated over a window size of 30 days to smooth out fluctuations in the stock prices.
# The actual closing prices and the moving average are both plotted for comparison.

"""

window_size = 30  # 30-day moving average
df['Moving Average'] = df['Close'].rolling(window=window_size).mean()

plt.figure(figsize=(12, 6))
plt.plot(df['Date'], df['Close'], label='Actual Closing Prices', alpha=0.7)
plt.plot(df['Date'], df['Moving Average'], label=f'{window_size}-Day Moving Average', color='orange', linewidth=2)
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Actual Closing Prices and Moving Average')
plt.legend()
plt.show()

"""## Cell 15: Explained Variance Ratio for PCA
# This cell prints the explained variance ratio for the PCA components.
# The explained variance ratio shows how much of the variance in the data is explained by each principal component.

"""

print("--- PCA Explained Variance Ratio ---")
print("Explained Variance Ratio:", pca.explained_variance_ratio_)

"""## Cell 16: Summary of Results
# In this final cell, we summarize the performance of both models (Linear Regression and SVM) by printing the MSE and R² scores.

"""

print("--- Summary of Model Results ---")
print(f"Linear Regression: MSE={lr_mse}, R²={lr_r2}")
print(f"SVM: MSE={svm_mse}, R²={svm_r2}")